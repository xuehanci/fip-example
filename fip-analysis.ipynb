{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b118da-497c-4326-b47e-f37f0f3c66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIP CSV Data Analysis - Jupyter Notebook Version\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a3299-bed4-47ca-84ba-98182f0741c3",
   "metadata": {},
   "source": [
    "Find FIB Folder and CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debf0ed-8d02-4c08-aeca-fc95a5242cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically find the 'fib' folder within /data directory\n",
    "data_base = \"/data\"\n",
    "fib_folder = None\n",
    "results_path = \"/results\"\n",
    "\n",
    "# Search for 'fib' folder\n",
    "for root, dirs, files in os.walk(data_base):\n",
    "    if 'fib' in dirs:\n",
    "        fib_folder = os.path.join(root, 'fib')\n",
    "        break\n",
    "\n",
    "if fib_folder is None:\n",
    "    print(\"‚ùå No 'fib' folder found in /data directory\")\n",
    "    # List what's available\n",
    "    print(\"Available directories in /data:\")\n",
    "    for item in os.listdir(data_base):\n",
    "        if os.path.isdir(os.path.join(data_base, item)):\n",
    "            print(f\"  üìÅ {item}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found fib folder: {fib_folder}\")\n",
    "    \n",
    "    # Find all CSV files in the fib folder\n",
    "    csv_files = glob.glob(os.path.join(fib_folder, \"*.csv\"))\n",
    "    print(f\"üìä Found {len(csv_files)} CSV files:\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"  üìÑ {os.path.basename(csv_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c0c1e-a3fd-4fdc-a392-c719c42ec9c5",
   "metadata": {},
   "source": [
    "Find FIB Folder and CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a4013-0c18-46c9-b01e-101fc4e53457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files into a dictionary\n",
    "data_dict = {}\n",
    "\n",
    "if fib_folder and csv_files:\n",
    "    for csv_file in csv_files:\n",
    "        file_name = os.path.basename(csv_file)\n",
    "        print(f\"Loading: {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            data_dict[file_name] = df\n",
    "            print(f\"  ‚úÖ Shape: {df.shape}, Columns: {list(df.columns)[:3]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error loading {file_name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"üéâ Successfully loaded {len(data_dict)} CSV files\")\n",
    "else:\n",
    "    print(\"‚ùå No CSV files to load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2692f-0b37-4517-8596-abbdf1ba5e71",
   "metadata": {},
   "source": [
    "Data Overview and Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb49f9-f1a4-4efd-8f8b-163dbf730dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about each CSV file\n",
    "print(\"üìã Dataset Overview:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for file_name, df in data_dict.items():\n",
    "    print(f\"üìÑ {file_name}\")\n",
    "    print(f\"   Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"   Numeric columns: {list(numeric_cols)}\")\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Basic statistics for the main signal (usually second numeric column)\n",
    "        if len(numeric_cols) > 1:\n",
    "            signal_col = numeric_cols[1]  # Skip time/index column\n",
    "        else:\n",
    "            signal_col = numeric_cols[0]\n",
    "            \n",
    "        signal_data = df[signal_col].dropna()\n",
    "        \n",
    "        # Calculate FIP-specific metrics\n",
    "        baseline = np.percentile(signal_data, 10)  # Bottom 10% as baseline\n",
    "        peak_thresh = np.percentile(signal_data, 95)  # Top 5% as peaks\n",
    "        \n",
    "        print(f\"   Signal column: {signal_col}\")\n",
    "        print(f\"   Range: [{signal_data.min():.3f}, {signal_data.max():.3f}]\")\n",
    "        print(f\"   Mean ¬± STD: {signal_data.mean():.3f} ¬± {signal_data.std():.3f}\")\n",
    "        print(f\"   Baseline estimate: {baseline:.3f}\")\n",
    "        print(f\"   Peak threshold: {peak_thresh:.3f}\")\n",
    "        \n",
    "        # Store analysis results\n",
    "        analysis_results[file_name] = {\n",
    "            'shape': df.shape,\n",
    "            'signal_column': signal_col,\n",
    "            'mean': signal_data.mean(),\n",
    "            'std': signal_data.std(),\n",
    "            'baseline': baseline,\n",
    "            'peak_threshold': peak_thresh,\n",
    "            'data_length': len(signal_data)\n",
    "        }\n",
    "\n",
    "print(f\"üìä Analysis complete for {len(analysis_results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f3177-d12d-4395-8f3f-7d5328fbc26b",
   "metadata": {},
   "source": [
    "Overview Visualizations - All Files Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21564d1c-cdcb-47d3-bcca-3b40ae1113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview plots comparing all CSV files\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('FIP Dataset Overview - All CSV Files', fontsize=16)\n",
    "\n",
    "# Generate colors for each file\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(data_dict)))\n",
    "\n",
    "# Plot 1: All signals overlaid\n",
    "for i, (file_name, df) in enumerate(data_dict.items()):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) >= 2:\n",
    "        axes[0,0].plot(df.iloc[:, 0], df.iloc[:, 1], \n",
    "                      alpha=0.7, color=colors[i], \n",
    "                      label=file_name[:15] + ('...' if len(file_name) > 15 else ''))\n",
    "\n",
    "axes[0,0].set_title('All Signals Overlay')\n",
    "axes[0,0].set_xlabel('Time/Sample')\n",
    "axes[0,0].set_ylabel('Signal')\n",
    "axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 2: Signal distributions (box plot)\n",
    "all_signals = []\n",
    "file_labels = []\n",
    "\n",
    "for file_name, df in data_dict.items():\n",
    "    if file_name in analysis_results:\n",
    "        signal_col = analysis_results[file_name]['signal_column']\n",
    "        signal_data = df[signal_col].dropna()\n",
    "        # Sample data if too large (for plotting performance)\n",
    "        if len(signal_data) > 10000:\n",
    "            signal_data = signal_data.sample(n=10000)\n",
    "        all_signals.extend(signal_data.values)\n",
    "        file_labels.extend([file_name[:10]] * len(signal_data))\n",
    "\n",
    "if all_signals:\n",
    "    signal_df = pd.DataFrame({'Signal': all_signals, 'File': file_labels})\n",
    "    sns.boxplot(data=signal_df, x='File', y='Signal', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Signal Distributions by File')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Statistics comparison heatmap\n",
    "if analysis_results:\n",
    "    stats_data = []\n",
    "    file_names = []\n",
    "    \n",
    "    for file_name, analysis in analysis_results.items():\n",
    "        stats_data.append([\n",
    "            analysis['mean'],\n",
    "            analysis['std'], \n",
    "            analysis['baseline'],\n",
    "            analysis['peak_threshold']\n",
    "        ])\n",
    "        file_names.append(file_name[:15])\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data, \n",
    "                           columns=['Mean', 'Std', 'Baseline', 'Peak_Thresh'],\n",
    "                           index=file_names)\n",
    "    sns.heatmap(stats_df, annot=True, fmt='.2f', ax=axes[1,0], cmap='viridis')\n",
    "    axes[1,0].set_title('Signal Statistics Heatmap')\n",
    "\n",
    "# Plot 4: File sizes comparison\n",
    "if analysis_results:\n",
    "    files = list(analysis_results.keys())\n",
    "    sizes = [analysis_results[f]['data_length'] for f in files]\n",
    "    \n",
    "    axes[1,1].bar(range(len(files)), sizes)\n",
    "    axes[1,1].set_title('Data Length by File')\n",
    "    axes[1,1].set_xlabel('File Index')\n",
    "    axes[1,1].set_ylabel('Number of Data Points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_path}/fip_overview_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebb197-e7c0-47c9-aa30-a4235f1a866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first file for detailed analysis (you can change this)\n",
    "if data_dict:\n",
    "    selected_file = list(data_dict.keys())[1]\n",
    "    df = data_dict[selected_file]\n",
    "    \n",
    "    print(f\"üîç Detailed analysis of: {selected_file}\")\n",
    "    \n",
    "    # Create detailed plots for selected file\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'Detailed Analysis: {selected_file}', fontsize=14)\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) >= 2:\n",
    "        time_col = df.iloc[:, 0]\n",
    "        signal_col = df.iloc[:, 1]\n",
    "        analysis = analysis_results[selected_file]\n",
    "        \n",
    "        # Raw signal with baseline and threshold lines\n",
    "        axes[0,0].plot(time_col, signal_col, color='blue', alpha=0.8)\n",
    "        axes[0,0].axhline(y=analysis['baseline'], color='green', \n",
    "                         linestyle='--', label=f\"Baseline: {analysis['baseline']:.2f}\")\n",
    "        axes[0,0].axhline(y=analysis['peak_threshold'], color='red', \n",
    "                         linestyle='--', label=f\"Peak Thresh: {analysis['peak_threshold']:.2f}\")\n",
    "        axes[0,0].set_title('Raw Signal with Thresholds')\n",
    "        axes[0,0].set_xlabel('Time/Sample')\n",
    "        axes[0,0].set_ylabel('Signal')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        # Signal histogram\n",
    "        axes[0,1].hist(signal_col.dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0,1].axvline(x=analysis['mean'], color='red', linestyle='-', \n",
    "                         label=f\"Mean: {analysis['mean']:.2f}\")\n",
    "        axes[0,1].set_title('Signal Distribution')\n",
    "        axes[0,1].set_xlabel('Signal Value')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Moving average (if enough data points)\n",
    "        if len(signal_col) > 100:\n",
    "            window = max(len(signal_col) // 50, 10)\n",
    "            moving_avg = signal_col.rolling(window=window, center=True).mean()\n",
    "            \n",
    "            axes[1,0].plot(time_col, signal_col, alpha=0.3, color='gray', label='Raw')\n",
    "            axes[1,0].plot(time_col, moving_avg, color='blue', linewidth=2, label=f'Moving Avg ({window})')\n",
    "            axes[1,0].set_title('Signal with Moving Average')\n",
    "            axes[1,0].set_xlabel('Time/Sample')\n",
    "            axes[1,0].set_ylabel('Signal')\n",
    "            axes[1,0].legend()\n",
    "        \n",
    "        # Signal changes (derivative)\n",
    "        signal_diff = np.diff(signal_col.dropna())\n",
    "        axes[1,1].plot(time_col[1:], signal_diff, color='purple', alpha=0.7)\n",
    "        axes[1,1].set_title('Signal Rate of Change')\n",
    "        axes[1,1].set_xlabel('Time/Sample')\n",
    "        axes[1,1].set_ylabel('Œî Signal')\n",
    "        axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_path}/detailed_{selected_file.replace(\".csv\", \"\")}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6f9d2-b55d-4469-8632-66c9c8e53b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed versions of each CSV file and save results\n",
    "print(\"üíæ Processing and saving data...\")\n",
    "\n",
    "for file_name, df in data_dict.items():\n",
    "    if file_name in analysis_results:\n",
    "        analysis = analysis_results[file_name]\n",
    "        signal_col = analysis['signal_column']\n",
    "        \n",
    "        # Create processed version with additional columns\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Add z-score normalization\n",
    "        processed_df[f'{signal_col}_zscore'] = (df[signal_col] - analysis['mean']) / analysis['std']\n",
    "        \n",
    "        # Add baseline-subtracted signal\n",
    "        processed_df[f'{signal_col}_baseline_subtracted'] = df[signal_col] - analysis['baseline']\n",
    "        \n",
    "        # Add peak detection (simple threshold-based)\n",
    "        processed_df[f'{signal_col}_is_peak'] = df[signal_col] > analysis['peak_threshold']\n",
    "        \n",
    "        # Save processed CSV\n",
    "        safe_filename = file_name.replace('.csv', '').replace(' ', '_')\n",
    "        processed_df.to_csv(f'{results_path}/processed_{safe_filename}.csv', index=False)\n",
    "        print(f\"  ‚úÖ Saved: processed_{safe_filename}.csv\")\n",
    "\n",
    "# Save comprehensive analysis summary\n",
    "summary = {\n",
    "    'experiment_info': {\n",
    "        'fib_folder_path': fib_folder,\n",
    "        'total_csv_files': len(data_dict),\n",
    "        'analysis_timestamp': datetime.now().isoformat()\n",
    "    },\n",
    "    'files_analyzed': list(data_dict.keys()),\n",
    "    'analysis_results': {k: {\n",
    "        'shape': v['shape'],\n",
    "        'signal_stats': {\n",
    "            'mean': float(v['mean']),\n",
    "            'std': float(v['std']),\n",
    "            'baseline': float(v['baseline']),\n",
    "            'peak_threshold': float(v['peak_threshold'])\n",
    "        }\n",
    "    } for k, v in analysis_results.items()}\n",
    "}\n",
    "\n",
    "with open(f'{results_path}/fip_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"üéâ Analysis Complete!\")\n",
    "print(f\"üìÅ Results saved to: {results_path}\")\n",
    "print(f\"üìä Files generated:\")\n",
    "print(f\"  - fip_overview_analysis.png (overview plots)\")\n",
    "print(f\"  - detailed_*.png (individual file analysis)\")\n",
    "print(f\"  - processed_*.csv (enhanced CSV files)\")\n",
    "print(f\"  - fip_analysis_summary.json (analysis metadata)\")\n",
    "print(f\"\\n‚ú® Ready for further analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f414a-f2ad-4b0d-8dbe-37f8bf5f75ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
